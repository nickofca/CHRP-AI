{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Script for CHRP-AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data to be cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filepath to data to be cleaned\n",
    "filepath = '../data/chillerLoad.csv'\n",
    "# read data from csv into pandas dataframe\n",
    "data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dictionary for unreliable indecies\n",
    "unreliable_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get index of UNRELIABLE points\n",
    "##### Store indecies of unreliable data points in dictionary where keys are column indecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indecies for all \"eDNA Status as String\" columns\n",
    "status_cols = [i for i in range(len(data.columns)) if i%3 == 2]\n",
    "# chiller_cols = [i for i in range(len(data.columns)) if i%3 == 0]\n",
    "# chiller_col_labels = [data.columns[x] for x in chiller_cols]\n",
    "\n",
    "# Loop through all status columns \n",
    "for j in status_cols:\n",
    "    unr_index = [] # reset unreliable index list to empty\n",
    "    for i in range(len(data)): # loop through all rows of data\n",
    "        if data.iloc[i,j] == 'UNRELIABLE': # check if unreliable\n",
    "            unr_index.append(i) # if unreliable, add index to list\n",
    "    unreliable_dict[j] = unr_index # assign list of unreliable indecies to column index key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "# Count number of UNRELIABLE points\n",
    "total = 0\n",
    "for x in unreliable_dict.keys():\n",
    "    total += len(unreliable_dict[x])\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use Linear Interpolation to get values for previously UNRELIABLE data points**\n",
    "Assuming that UNRELIABLE data points occur in bunches, we want to linearly interpolate using the boundary values of Reliable data points. The \"before bound\" will be the value of the Reliable data point which occured right before the bunch of UNRELIABLE data points. The \"after bound\" will be the value of the Reliable data point which occurs right after the bunch of UNRELIABLE data points.\n",
    "\n",
    "Process for Linear Interpolation: <br>\n",
    "    1) Find \"before index\" --> Index of previous Reliable data point <br>\n",
    "    2) Find \"before bound\" --> Value of previous Reliable data point <br>\n",
    "    3) Find \"after index\" --> Index of next Reliable data point <br>\n",
    "    4) Find \"after bound\" --> Value of next Reliable data point <br>\n",
    "    5) Calculate slope --> $slope = \\frac{a_{bound}-b_{bound}}{a_{index}-b_{index}}$ <br>\n",
    "    6) Calculate new value --> $b_{bound} + slope * (c_{index} - b_{index})$ <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all column indecies with unreliable data\n",
    "for x in unreliable_dict.keys():\n",
    "    # loop through all unreliable values\n",
    "    for i in unreliable_dict[x]:\n",
    "        # loop through all indecies before unreliable point\n",
    "        for j in range(1,i):\n",
    "            if data.iloc[i-j,x] != 'UNRELIABLE': # check for unreliable point\n",
    "                # IF NOT UNRELIABLE: \n",
    "                before_bound = data.iloc[i-j,x-1] # set before bound value\n",
    "                before_index = i-j # set before index value\n",
    "                break # break loop once before bound is found\n",
    "        # loop through all indecies after unreliable point\n",
    "        for j in range(1,len(data) - i): \n",
    "            if data.iloc[i+j,x] != 'UNRELIABLE': # check for unreliable point\n",
    "                # IF NOT UNRELIABLE:\n",
    "                after_bound = data.iloc[i+j,x-1] # set after bound value\n",
    "                after_index = i+j # set after index value\n",
    "                break # break loop once after bound is found\n",
    "       \n",
    "        # Linear Interpolation to substitute for UNRELIABLE values\n",
    "        slope = (after_bound - before_bound)/(after_index - before_index)\n",
    "        data.iloc[i,x-1] = before_bound + slope * (i - before_index)\n",
    "\n",
    "# Replace 'UNRELIABLE' tags with 'LI' tags\n",
    "for x in unreliable_dict.keys():\n",
    "    for i in unreliable_dict[x]:\n",
    "        data.iloc[i,x] = 'LI'\n",
    "        \n",
    "        # DEBUG\n",
    "#         if (x == 32 and i == 354):\n",
    "#             print('before bound: {0}'.format(before_bound))\n",
    "#             print('before index: {0}'.format(before_index))\n",
    "#             print('after bound: {0}'.format(after_bound))\n",
    "#             print('after index: {0}'.format(after_index))\n",
    "#             print('slope: {0}'.format(slope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "# Count number of LI tags\n",
    "total = 0\n",
    "for j in status_cols:\n",
    "    for i in range(len(data)):\n",
    "        if data.iloc[i,j] == 'LI':\n",
    "            total += 1\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset with 'LI' tags as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../data/Load_Data_Full_LItags.csv'\n",
    "data.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep relevant columns and rename accordingly\n",
    "Keep only the first Date/time column and all Values columns. Rename Values columns according to chiller name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rel_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-517bbe47b190>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mrename_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchiller_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Assign new column names (appropriate chiller names)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mrel_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrename_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rel_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Get chiller names\n",
    "chiller_cols = [i for i in range(len(data.columns)) if i%3 == 0]\n",
    "chiller_cols = [data.columns[x] for x in chiller_cols]\n",
    "# Get value columns to keep\n",
    "value_cols = [i for i in range(len(data.columns)) if i%3 == 1]\n",
    "value_cols = [data.columns[x] for x in value_cols]\n",
    "# Compile columns to keep in dataframe\n",
    "data.rename({'AHSC_CHL01_TONS':'Date/Time'},axis = 1,inplace = True)\n",
    "keep_cols = ['Date/Time'] + value_cols\n",
    "# Create Dictionary for guide to rename columns with appropriate chiller names\n",
    "rename_dict = {}\n",
    "for i in range(len(keep_cols[1:])):\n",
    "    rename_dict[keep_cols[i]] = chiller_cols[i]\n",
    "# Assign new column names (appropriate chiller names)\n",
    "rel_data.rename(rename_dict,axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save CSV of relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../data/Load_Data_Full_ChillerValuesOnly.csv'\n",
    "rel_data.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Outliers\n",
    "Use 1 hour interval (15 points before and 15 points after current point)\n",
    "\n",
    "For points with only N points before (w/ N < 15), use N points before and 30-N points after.<br>\n",
    "For points with only N points after (w/ N < 15), use N points after and 30-N points before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols = []\n",
    "for x in rel_data.columns[1:]:\n",
    "    outlier_col = [0 for i in range(len(rel_data))]\n",
    "    for i in range(len(rel_data)):\n",
    "        if i < 15:\n",
    "            window = rel_data.loc[0:29,x]\n",
    "            q1 = window.quantile(0.25)\n",
    "            q3 = window.quantile(0.75)\n",
    "            iqr = abs(q3 - q1)\n",
    "            if ((rel_data.loc[i,x] < q1 - 1.5 * iqr) or (rel_data.loc[i,x] > q3 + 1.5 * iqr)):\n",
    "                outlier_col[i] = 1\n",
    "        elif i > len(rel_data) - 15:\n",
    "            window = rel_data.loc[len(rel_data) - 30:len(rel_data) - 1,x]\n",
    "            q1 = window.quantile(0.25)\n",
    "            q3 = window.quantile(0.75)\n",
    "            iqr = abs(q3 - q1)\n",
    "            if ((rel_data.loc[i,x] < q1 - 1.5 * iqr) or (rel_data.loc[i,x] > q3 + 1.5 * iqr)):\n",
    "                outlier_col[i] = 1\n",
    "        else:\n",
    "            window = rel_data.loc[i-15:i+14,x]\n",
    "            q1 = window.quantile(0.25)\n",
    "            q3 = window.quantile(0.75)\n",
    "            iqr = abs(q3 - q1)\n",
    "            if ((rel_data.loc[i,x] < q1 - 1.5 * iqr) or (rel_data.loc[i,x] > q3 + 1.5 * iqr)):\n",
    "                outlier_col[i] = 1\n",
    "    outlier_cols.append(outlier_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
